# ğŸ”´ Red-Team Harness

## ğŸš€ Purpose

This source code is designed to conduct safety red-teaming tests on large language models (LLMs).

- ğŸ§ª Run safety scenario testing tasks: _reward hacking_, _deception_, _policy refusal_... (Currently developing _reward hacking_)
- ğŸ“œ Automatically logs output, reasoning (if available), metadata â†’ making it easier to analyze model behavior.
- ğŸ“‚ Exports findings in JSON format for audit, reporting, or AI Safety research.

## ğŸ›¡ï¸ Safety

âš ï¸ **Important Notice**: This source code is intended **for educational and research purposes only**.
Do not use it for illegal or unethical activities.
Files inside the `samples` directory may contain unfiltered content generated by LLMs, which could be disturbing to some readers.

## ğŸ“¦ Quick Start

```bash
# 0) Clone repo
git clone https://github.com/thang-la/red-team-harness.git
cd red-team-harness

# 1) Setup env
./setup.sh

# 2) Activate venv
source .venv/bin/activate && pip install .

# 3) Run a red-team task
rethar run --task reward_hacking --model openai/gpt-oss-20b --temperature 0.7 --repeat 10

# 4) Inspect outputs
ls findings/
```

## ğŸ“š Documentation

Additional documentation can be found in the **`/docs`** directory.
Recommended starting point: **[`/docs/index.md`](./docs/index.md)**
