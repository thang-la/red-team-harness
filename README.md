# 🔴 Red-Team Harness

## 🚀 Purpose

This source code is designed to conduct safety red-teaming tests on large language models (LLMs).

- 🧪 Run safety scenario testing tasks: _reward hacking_, _deception_, _policy refusal_... (Currently developing _reward hacking_)
- 📜 Automatically logs output, reasoning (if available), metadata → making it easier to analyze model behavior.
- 📂 Exports findings in JSON format for audit, reporting, or AI Safety research.

## 🛡️ Safety

⚠️ **Important Notice**: This source code is intended **for educational and research purposes only**.
Do not use it for illegal or unethical activities.
Files inside the `samples` directory may contain unfiltered content generated by LLMs, which could be disturbing to some readers.

## 📦 Quick Start

```bash
# 0) Clone repo
git clone https://github.com/thang-la/red-team-harness.git
cd red-team-harness

# 1) Setup env
./setup.sh

# 2) Activate venv
source .venv/bin/activate && pip install .

# 3) Run a red-team task
rethar run --task reward_hacking --model openai/gpt-oss-20b --temperature 0.7 --repeat 10

# 4) Inspect outputs
ls findings/
```

## 📚 Documentation

Additional documentation can be found in the **`/docs`** directory.
Recommended starting point: **[`/docs/index.md`](./docs/index.md)**
